# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# ===== CELL 1: Install Dependencies (Run Once) =====
# This cell installs required packages - takes about 1-2 minutes
!pip install -q transformers torch gradio accelerate bitsandbytes

# ===== CELL 2: Import Libraries =====
# This cell imports all necessary libraries - runs in seconds
import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import gc
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Libraries imported successfully!")
print(f"üî• PyTorch version: {torch.__version__}")
print(f"üöÄ CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üéØ GPU: {torch.cuda.get_device_name()}")

# ===== CELL 3: Initialize Model Class =====
# This cell sets up the model class - runs instantly
class QuickGraniteAI:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.loaded = False

    def load_model(self):
        """Quick model loading with optimizations"""
        if self.loaded:
            return "‚úÖ Model already loaded!"

        try:
            print("üì• Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                "ibm-granite/granite-3.2-2b-instruct",
                trust_remote_code=True
            )

            print("üß† Loading model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                "ibm-granite/granite-3.2-2b-instruct",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                load_in_8bit=True
            )

            # Set padding token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.loaded = True
            return "‚úÖ Model loaded successfully!"

        except Exception as e:
            return f"‚ùå Error: {str(e)}"

    def generate(self, prompt, max_len=300, temp=0.7):
        """Fast text generation"""
        if not self.loaded:
            return "‚ùå Load model first!"

        try:
            # Format prompt
            formatted = f"<|user|>\n{prompt}\n<|assistant|>\n"

            # Tokenize
            inputs = self.tokenizer(
                formatted,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids.to(self.model.device),
                    attention_mask=inputs.attention_mask.to(self.model.device),
                    max_new_tokens=max_len,
                    temperature=temp,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decode
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )

            return response.strip()

        except Exception as e:
            return f"‚ùå Generation error: {str(e)}"

# Initialize the AI app
ai = QuickGraniteAI()
print("üéØ AI app initialized!")

# ===== CELL 4: Load the Model =====
# This cell loads the model - takes about 2-3 minutes first time
print("üöÄ Loading IBM Granite 3.2 2B model...")
status = ai.load_model()
print(status)

# Check memory usage
if torch.cuda.is_available():
    memory_gb = torch.cuda.memory_allocated() / 1024**3
    print(f"üìä GPU Memory used: {memory_gb:.2f} GB")

# ===== CELL 5: Quick Test =====
# This cell tests the model - runs in seconds
if ai.loaded:
    test_prompt = "What is artificial intelligence?"
    print("üß™ Testing model with quick prompt...")
    start_time = time.time()
    response = ai.generate(test_prompt, max_len=100)
    end_time = time.time()

    print(f"‚ö° Response time: {end_time - start_time:.2f} seconds")
    print(f"ü§ñ Response: {response}")
else:
    print("‚ùå Model not loaded. Please run Cell 4 first.")

# ===== CELL 6: Simple Gradio Interface =====
# This cell creates a basic interface - runs instantly
def simple_chat(message, history, max_tokens, temperature):
    if not ai.loaded:
        return history + [["Please load the model first!", ""]]

    response = ai.generate(message, max_len=max_tokens, temp=temperature)
    history.append([message, response])
    return history

# Create simple interface
with gr.Blocks(title="Quick Granite AI") as demo:
    gr.Markdown("# üß† IBM Granite 3.2 2B Quick Chat")

    chatbot = gr.Chatbot(height=300)
    msg = gr.Textbox(label="Message", placeholder="Type here...")

    with gr.Row():
        max_tokens = gr.Slider(50, 500, 200, label="Max Tokens")
        temperature = gr.Slider(0.1, 1.5, 0.7, label="Temperature")

    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear")

    send_btn.click(simple_chat, [msg, chatbot, max_tokens, temperature], chatbot)
    clear_btn.click(lambda: [], outputs=chatbot)
    msg.submit(simple_chat, [msg, chatbot, max_tokens, temperature], chatbot)

print("üé® Simple interface ready!")

# ===== CELL 7: Launch the App =====
# This cell launches the Gradio interface - runs instantly
demo.launch(share=True, debug=False)

# ===== CELL 8: Advanced Features (Optional) =====
# This cell adds more advanced functionality - runs instantly

def summarize_text(text):
    """Quick summarization"""
    prompt = f"Summarize this text concisely:\n\n{text}\n\nSummary:"
    return ai.generate(prompt, max_len=150, temp=0.3)

def answer_question(context, question):
    """Quick Q&A"""
    prompt = f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"
    return ai.generate(prompt, max_len=200, temp=0.3)

def generate_code(description, language="Python"):
    """Quick code generation"""
    prompt = f"Write {language} code for: {description}\n\nCode:"
    return ai.generate(prompt, max_len=400, temp=0.2)

# Test advanced features
if ai.loaded:
    print("üîß Advanced features ready!")

    # Quick examples
    print("\nüìù Quick Summary Test:")
    sample_text = "Artificial intelligence is transforming industries worldwide. Machine learning algorithms can now process vast amounts of data to identify patterns and make predictions. This technology is being applied in healthcare, finance, transportation, and many other sectors."
    summary = summarize_text(sample_text)
    print(f"Summary: {summary[:100]}...")

    print("\nüíª Quick Code Test:")
    code = generate_code("function to calculate factorial")
    print(f"Code: {code[:100]}...")

# ===== CELL 9: Full Featured Interface (Optional) =====
# This cell creates the complete interface - runs instantly

def create_full_interface():
    with gr.Blocks(theme=gr.themes.Soft()) as full_app:
        gr.Markdown("# üß† IBM Granite 3.2 2B - Full Featured AI Assistant")

        with gr.Tabs():
            # Chat Tab
            with gr.TabItem("üí¨ Chat"):
                chatbot = gr.Chatbot(height=400)
                msg = gr.Textbox(label="Message")
                with gr.Row():
                    send = gr.Button("Send", variant="primary")
                    clear = gr.Button("Clear")
                    max_tok = gr.Slider(50, 800, 300, label="Max Tokens")
                    temp = gr.Slider(0.1, 1.5, 0.7, label="Temperature")

                send.click(simple_chat, [msg, chatbot, max_tok, temp], chatbot)
                clear.click(lambda: [], outputs=chatbot)

            # Text Tools Tab
            with gr.TabItem("üìù Text Tools"):
                with gr.Row():
                    with gr.Column():
                        tool_type = gr.Dropdown(
                            ["Summarize", "Q&A", "Code", "Creative"],
                            label="Tool Type"
                        )
                        input_text = gr.Textbox(
                            label="Input Text",
                            lines=6,
                            placeholder="Enter your text here..."
                        )
                        extra_input = gr.Textbox(
                            label="Extra Input (Question/Language/Style)",
                            placeholder="Optional: question, programming language, or style"
                        )
                        process_btn = gr.Button("Process", variant="primary")

                    with gr.Column():
                        output_text = gr.Textbox(
                            label="Output",
                            lines=12,
                            interactive=False
                        )

                def process_text(tool, text, extra):
                    if not ai.loaded:
                        return "‚ùå Load model first!"

                    if tool == "Summarize":
                        return summarize_text(text)
                    elif tool == "Q&A":
                        return answer_question(text, extra or "What is this about?")
                    elif tool == "Code":
                        return generate_code(text, extra or "Python")
                    elif tool == "Creative":
                        style = extra or "story"
                        prompt = f"Write a {style}: {text}"
                        return ai.generate(prompt, max_len=600, temp=0.8)

                process_btn.click(
                    process_text,
                    [tool_type, input_text, extra_input],
                    output_text
                )

        gr.Markdown("‚ö° **Quick and efficient AI powered by IBM Granite 3.2 2B**")

    return full_app

# Uncomment the next two lines to launch the full interface
# full_interface = create_full_interface()
# full_interface.launch(share=True)

print("üéâ All cells ready! Choose your interface:")
print("üì± Simple: Already launched above")
print("üîß Full: Uncomment lines in Cell 9")

# ===== CELL 10: Quick Utilities =====
# This cell provides quick utility functions - runs instantly

def quick_generate(prompt, tokens=200):
    """Super quick generation function"""
    if ai.loaded:
        return ai.generate(prompt, max_len=tokens, temp=0.7)
    return "‚ùå Model not loaded"

def memory_status():
    """Check memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        return f"üîã Allocated: {allocated:.2f}GB | Cached: {cached:.2f}GB"
    return "üíª Running on CPU"

def clear_memory():
    """Clear GPU memory"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    return "üßπ Memory cleared!"

# Quick usage examples
print("üîß Utility functions loaded:")
print("‚Ä¢ quick_generate('your prompt')")
print("‚Ä¢ memory_status()")
print("‚Ä¢ clear_memory()")

# Test utilities
print(f"\n{memory_status()}")

# ===== USAGE INSTRUCTIONS =====
print("""
üöÄ QUICK START GUIDE:

1. Run Cell 1: Install packages (1-2 minutes)
2. Run Cell 2: Import libraries (5 seconds)
3. Run Cell 3: Initialize class (instant)
4. Run Cell 4: Load model (2-3 minutes first time)
5. Run Cell 5: Test model (5-10 seconds)
6. Run Cell 6: Create simple interface (instant)
7. Run Cell 7: Launch app (instant)

üìù QUICK EXAMPLES:
- quick_generate("Write a poem about coding")
- quick_generate("Explain machine learning simply")
- quick_generate("Create a Python function for sorting")

üí° TIPS:
- Model loads once, then generates quickly (1-3 seconds)
- Use shorter prompts for faster responses
- Lower max_tokens = faster generation
- Temperature 0.3 = focused, 0.8 = creative
""")

# ==============================
# PERSONAL FINANCE AI - IBM Granite
# Complete Code for Google Colab + Gradio
# ==============================

# ===== CELL 1: Install & Import =====
!pip install -q transformers accelerate gradio bitsandbytes plotly

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import json
import re
import time
import gc
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Libraries imported successfully!")
print(f"üî• PyTorch: {torch.__version__}")
print(f"üöÄ CUDA: {torch.cuda.is_available()}")

# ===== CELL 2: Load Hugging Face Model =====
model_id = "ibm-granite/granite-3.2-2b-instruct"

print("‚è≥ Loading model... this may take 1‚Äì2 minutes.")
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_8bit=True,
    torch_dtype=torch.float16
)
print("‚úÖ Model loaded successfully!")

# ===== CELL 3: Finance AI Class =====
class PersonalFinanceAI:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.user_profiles = {}

    def create_user_profile(self, name, age, income, expenses, savings_goal, user_type="general"):
        self.user_profiles[name] = {
            "age": age,
            "income": income,
            "expenses": expenses,
            "savings_goal": savings_goal,
            "user_type": user_type,
            "created": datetime.now().isoformat()
        }
        return f"‚úÖ Profile created for {name}!"

    def generate_response(self, user, prompt, max_tokens=300):
        if not user or user not in self.user_profiles:
            return "‚ùå Please create your profile first!"

        # Enforce education-only disclaimer
        full_prompt = (
            "You are a personal finance educational assistant. "
            "Only answer questions related to education, personal finance, saving, investment, and money management. "
            "If the question is unrelated, reply with: '‚ö†Ô∏è Please ask only educational or finance-related questions.'\n\n"
            f"User: {prompt}\nAssistant:"
        )

        inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).split("Assistant:")[-1].strip()

# ===== CELL 4: Initialize Finance AI =====
finance_ai = PersonalFinanceAI(model, tokenizer)

# ===== CELL 5: Learning Center & Quiz Generator =====
def get_learning_content(user, topic):
    if not user:
        return "‚ùå Please create your profile first!"

    profile = finance_ai.user_profiles.get(user, {})
    user_type = profile.get("user_type", "general")
    age = profile.get("age", 30)
    income = profile.get("income", 0)

    prompt = f"""Create an educational lesson on '{topic}' tailored for a {user_type} ({age} years old).
Include:
1. Key concept explanations
2. Practical examples for income ${income:,.0f}
3. Step-by-step actions
4. Common pitfalls
5. Recommended resources
Always include a disclaimer: "This is for educational purposes only."
"""
    return finance_ai.generate_response(user, prompt, max_tokens=600)

def generate_quiz(user, topic, num_questions=5, difficulty="Medium"):
    if not user:
        return "‚ùå Please create your profile first!"

    prompt = f"""Generate {num_questions} multiple-choice quiz questions on '{topic}'.
- 4 options (A-D)
- Mark the correct answer
- Provide a 1-line explanation
Difficulty: {difficulty}
Always include a disclaimer: "This quiz is for educational purposes only."
"""
    return finance_ai.generate_response(user, prompt, max_tokens=400)

# ===== CELL 6: Gradio Interface =====
def create_finance_interface():
    with gr.Blocks(theme=gr.themes.Soft(), title="Personal Finance AI") as app:
        gr.Markdown("# üè¶ Personal Finance AI Advisor\n### Powered by IBM Granite 3.2 2B\nFor **educational purposes only.**")
        current_user = gr.State("")

        with gr.Tabs():
            # Profile Tab
            with gr.TabItem("üë§ Profile Setup"):
                name = gr.Textbox(label="Name")
                age = gr.Number(label="Age", value=25)
                income = gr.Number(label="Monthly Income ($)", value=2000)
                expenses = gr.Number(label="Monthly Expenses ($)", value=1500)
                goal = gr.Textbox(label="Savings Goal")
                user_type = gr.Dropdown(["student", "professional", "retired", "general"], value="general", label="User Type")
                create_btn = gr.Button("Create Profile ‚úÖ")
                profile_output = gr.Textbox(label="Status")
                create_btn.click(
                    lambda n,a,i,e,g,u: finance_ai.create_user_profile(n,a,i,e,g,u),
                    [name,age,income,expenses,goal,user_type],
                    profile_output
                ).then(lambda n: n, name, current_user)

            # Advisor Tab
            with gr.TabItem("üí¨ Chat Advisor"):
                query = gr.Textbox(label="Your Question", placeholder="Ask about budgeting, saving, or investments...")
                ask_btn = gr.Button("Get Advice")
                response = gr.Textbox(label="Response", lines=10)
                ask_btn.click(lambda u,q: finance_ai.generate_response(u,q), [current_user,query], response)

            # Learning Center
            with gr.TabItem("üìö Learning Center"):
                topic = gr.Dropdown(
                    ["Basics of Personal Finance","Investment Fundamentals","Tax Planning","Budgeting","Retirement Planning"],
                    value="Basics of Personal Finance", label="Choose Topic")
                learn_btn = gr.Button("Get Lesson üìñ")
                lesson_out = gr.Textbox(lines=12, label="Lesson")
                learn_btn.click(get_learning_content, [current_user, topic], lesson_out)

                gr.Markdown("### üìù Quiz Generator")
                quiz_topic = gr.Textbox(label="Quiz Topic", placeholder="e.g., Investment Fundamentals")
                num_q = gr.Slider(1,10,5,label="Number of Questions")
                diff = gr.Dropdown(["Easy","Medium","Hard"], value="Medium", label="Difficulty")
                quiz_btn = gr.Button("Generate Quiz")
                quiz_out = gr.Textbox(lines=12, label="Quiz")
                quiz_btn.click(generate_quiz, [current_user, quiz_topic, num_q, diff], quiz_out)

        return app

app = create_finance_interface()
app.launch(share=True)







